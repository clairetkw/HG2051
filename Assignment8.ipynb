{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8\n",
    "\n",
    "**due:** Wednesday, 06 November\n",
    "\n",
    "This assignment gives you more practice with part-of-speech tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = 'Teo Kai Wen'\n",
    "COLLABORATORS = 'Ser Han'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 (2 points)\n",
    "\n",
    "Below I load the tagged sentences for the `news` category from the Brown corpus and store them in `brown_news`. You are to convert them from the `en-brown` tagset to the `universal` tagset using `nltk.tag.map_tag()`. Store the results in `unipos_news`. Note the following:\n",
    "\n",
    "* These are not lists of (word, tag) pairs, but lists of tagged sentences where each tagged sentence is a list of pairs; e.g., `[[(word1, tag1), (word2, tag2), ...], [...]]`\n",
    "* `nltk.tag.map_tag()`, as with `nltk.tag.str2tuple()`, works on a single item at a time; you'll need to apply it to each tag in each pair in each sentence in the list. This may be difficult to do with a single list comprehension, so you may want to use at least one regular for-loop and accumulate the results. Inspect the data structure first if you are having trouble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ec09154cb6df12836180200d31912b5b",
     "grade": true,
     "grade_id": "cell-092a44444f45955d",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')]\n",
      "\n",
      "[('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN'), ('said', 'VERB'), ('Friday', 'NOUN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP'), (\"Atlanta's\", 'NOUN'), ('recent', 'ADJ'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'VERB'), ('``', '.'), ('no', 'DET'), ('evidence', 'NOUN'), (\"''\", '.'), ('that', 'ADP'), ('any', 'DET'), ('irregularities', 'NOUN'), ('took', 'VERB'), ('place', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "## import nltk.tag\n",
    "import nltk.tag\n",
    "from nltk.corpus import brown\n",
    "brown_news = brown.tagged_sents(categories='news')\n",
    "\n",
    "unipos_news = []\n",
    "for i in range (0, (len(brown_news))):\n",
    "    newtagged = list(map(lambda x: (x[0], nltk.map_tag('en-brown', 'universal', x[1])), brown_news[i])) # YOUR CODE HERE\n",
    "    unipos_news.append(newtagged)\n",
    "                     \n",
    "print(brown_news[0])\n",
    "print()\n",
    "print(unipos_news[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 (2 points)\n",
    "\n",
    "Get an integer, `k`, that is 1/5 the size of `brown_news` and `unipos_news`. Use that integer to split both into `brown_test` and `brown_train`, `unipos_test` and `unipos_train`, respectively. For both, the `_test` sets should be 1/5 of the overall size and the `_train` sets should be 4/5 of the overall size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "aa50cb807f5c81a1cb8be7e1fee00bc2",
     "grade": true,
     "grade_id": "cell-a2d9d073a1d9a6c1",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "k = int(len(brown_news) / 5) \n",
    "brown_test = brown_news[:k]\n",
    "brown_train = brown_news[k:]\n",
    "unipos_test = unipos_news[:k]\n",
    "unipos_train = unipos_news[k:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 (2 points)\n",
    "\n",
    "Use `nltk.UnigramTagger` and `nltk.BigramTagger` to construct basic taggers from the `brown_train` and `unipos_train` data. Store them as `brown_unitagger` and `brown_bitagger`, `unipos_unitagger` and `unipos_bitagger`, respectively.\n",
    "\n",
    "For the taggers built from the `brown_train` data, use the provided `nn_tagger` as the backoff to `brown_unitagger` and `brown_unitagger` as the backoff to `brown_bitagger`.\n",
    "\n",
    "Likewise, for the `unipos_train` data, use the `noun_tagger` as the backoff to `unipos_unitagger` and `unipos_unitagger` as the backoff to `unipos_bitagger`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "69b381de002d626fbb90844860c573d6",
     "grade": true,
     "grade_id": "cell-57ff9006f832e462",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "nn_tagger = nltk.DefaultTagger('NN')\n",
    "brown_unitagger = nltk.UnigramTagger(brown_train, backoff=nn_tagger)\n",
    "brown_bitagger = nltk.BigramTagger(brown_train, backoff=brown_unitagger)\n",
    "\n",
    "noun_tagger = nltk.DefaultTagger('NOUN')\n",
    "unipos_unitagger = nltk.UnigramTagger(unipos_train, backoff=noun_tagger)\n",
    "unipos_bitagger = nltk.BigramTagger(unipos_train, backoff=unipos_unitagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 (2 point)\n",
    "\n",
    "Evaluate `brown_bitagger` (with `brown_bitagger.evaluate()`) on the `brown_test` data and `unipos_bitagger` on the `unipos_test` data. Print the results from evaluating with these taggers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "82fec7c2ac31cfa0a065c3b948b74ef7",
     "grade": true,
     "grade_id": "cell-d584c190db512488",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8549711604262391\n",
      "0.9325936064131392\n"
     ]
    }
   ],
   "source": [
    "print(brown_bitagger.evaluate(brown_test))\n",
    "print(unipos_bitagger.evaluate(unipos_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5 (1 point)\n",
    "\n",
    "You should find that the data with the universal tags gets a better score. Why do you think that is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "29f830b9a195a297222b639b47363056",
     "grade": true,
     "grade_id": "cell-047fa25f0cd59c22",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Maybe the universal tagging system is broader (that is, less specific) so more words can be assigned tags as opposed to the finer-grained brown tagset.\n",
    "\n",
    "Ans: More tags in brown = more opportunities to make mistakes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6 (1 point)\n",
    "\n",
    "In class we found that creating a bigram tagger without any backoff model did much worse than a unigram tagger. Why is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3bb1725993649f39d75bb704f60aad88",
     "grade": true,
     "grade_id": "cell-18fe7dec2e887322",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "In a bigram tagger word order is important, unlike in a unigram tagger, so when a bigram tagger encounters new sentences not found in the training data it is unable to assign a tag to them and, without a backoff model, those words would just remain untagged."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
